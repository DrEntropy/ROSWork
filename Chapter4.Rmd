# Chapter 4 Selected Exercises

The ones I plan to demonstrate in book club will be noted.

```{r}
library(tidyverse)
```


## Exercise 4.1

Comparison of proportions: A randomized experiment is performed within a survey. 1000 people are contacted. Half the people contacted are promised a $5 incentive to participate, and half are not promised an incentive. The result is a 50% response rate among the treated group and 40% response rate among the control group. Give an estimate and standard error of the average treatment effect.

---

The estimate is 10% effect, with standard error ~ 3%:

```{r}
sqrt((.5^2 + .4*.6)/500)

```



## Exercise 4.3

Comparison of proportions: You want to gather data to determine which of two students is a better basketball shooter. One of them shoots with 30% accuracy and the other is a 40% shooter. Each student takes 20 shots and you then compare their shooting percentages. 
What is the probability that the better shooter makes more shots in this small experiment?

---

I will use normal approximation.  The difference is 10% , so computing the standard error of this percentage:

```{r}
se <- sqrt((.7*.3 + .4*.6)/20)
1-pnorm(0, mean = 0.1, sd = se)
```

The standard error was  15% .   The probability that the bettter shooter makes more shots in this approximation is 75%

Let's sim to be sure the normal approximation is good. 

```{r}
p1 <- 0.3
p2 <- 0.4

trials <- 100000
shooter1 <- rbinom(trials,20,p1)
shooter2 <- rbinom(trials,20,p2)
mean(shooter2 > shooter1)
 
```

More like 70%. However I dont know a closed form for the difference of two binomial distributions so the numerical answer is the best I can get. Maybe someone has a better idea.

## Exercise 4.4

Designing an experiment: You want to gather data to determine which of two students is a better basketball shooter. You plan to have each student take # shots and then compare their shooting percentages. Roughly how large does # have to be for you to have a good chance of distinguishing a 30% shooter from a 40% shooter?

---

Here I will take 'a good chance' to mean 97.5% (1 tailed 2 standard deviations), so we need enough shots to get the 10% to be about 2 standard deviations. So we require that $\sqrt{.3*.7 + .4*.6}/\sqrt{n} \sim .10/2$ Solving we find n ~ 180

```{r}
n <-4*(0.3*0.7+0.4*0.6)/(.1^2)

```

check with sim again

```{r}
p1 <- 0.3
p2 <- 0.4
n <- 180

trials <- 100000
shooter1 <- rbinom(trials,n,p1)
shooter2 <- rbinom(trials,n,p2)
mean(shooter2 > shooter1)
 
```

## Exercise 4.6 - BOOK CLUB
Hypothesis testing: The following are the proportions of girl births in Vienna for each month in 1908 and 1909 (out of an average of 3900 births per month):

.4777  .4875  .4859  .4754  .4874  .4864  .4813  .4787  .4895  .4797  .4876  .4859
.4857  .4907  .5010  .4903  .4860  .4911  .4871  .4725  .4822  .4870  .4823  .4973

The data are in the folder Girls. These proportions were used by von Mises (1957) to support a claim that that the sex ratios were less variable than would be expected under the binomial distribution. We think von Mises was mistaken in that he did not account for the possibility that this discrepancy could arise just by chance.

(a)  Compute the standard deviation of these proportions and compare to the standard deviation that would be expected if the sexes of babies were independently decided with a constant probability over the 24-month period.

(b)  The observed standard deviation of the 24 proportions will not be identical to its theoretical expectation. In this case, is this difference small enough to be explained by random variation? Under the randomness model, the actual variance should have a distribution with expected value equal to the theoretical variance, and proportional to a $\chi^2$ random variable with 23 degrees of freedom; see page 53.

---

```{r}
girls <- as.numeric(readr::read_lines('girls.dat', skip=1))
sample_sd<-sd(girls)
sample_sd
```

```{r}
p_est <- mean(girls)
n_births <- 3900
expected_sd <- sqrt(p_est*(1-p_est)/n_births)
expected_sd 
```

Recall that this quantity:
$$
s_y^2*(n-1)/\sigma^2
$$

Is distributed with a $\chi^2$ distribution with n-1 degrees of freedom.  This quantity has the value:

```{r}
ratio <- sample_sd^2*23/expected_sd^2
ratio
```



```{r}
sds <- seq(.003,.015, .0002)
ratios <- sds^2*23/expected_sd^2
dist<- dchisq(ratios,23)
plot(sds,dist, type='l')
```


What is the probability that the standard deviation was as small as we observed or smaller?

```{r}
pchisq(ratio,23)
```

This is large enough that one should not be surprised at seeing a smaller variation then expected.

## Exercise 4.7 - BOOK CLUB

Inference from a proportion with $y = 0$: Out of a random sample of 50 Americans, zero report
having ever held political office. From this information, give a 95% confidence interval for the
proportion of Americans who have ever held political office.

---

```{r}
p_hat <- 2/(50+4)
se <- sqrt(p_hat*(1-p_hat)/50)
c(p_hat-2*se,p_hat+2*se)

```

So 'truncating' we say it is from 0 to 9%.  


## Exercise 4.9 - BOOK CLUB

Inference for a probability: A multiple-choice test item has four options. Assume that a student
taking this question either knows the answer or does a pure guess. A random sample of 100
students take the item, and 60% get it correct. Give an estimate and 95% confidence interval for
the percentage in the population who know the answer.

---

Suppose $f$ = percentage who know the answer.  Then we would expect to see $100f$ correct answers from those who know the answers and, on average, $100(1-f)/4$ extra correct answers from the lucky guessers, so that $100*(f + (1-f)/4)=60$ correct answers.  With algebra we find $\hat{f} = 0.467 $.

The standard error of this estimate is $\sqrt{f(1-f)/n}= \sqrt{(1/4)(3/4)/((1-f)*100)} \approx 0.06$ 

```{r}
se = sqrt(1/4*3/4/((1-.467)*100))
qnorm(c(.025,.975),.467,sd = se)
```

Or using binomial
```{r}
60-qbinom(c(.025,.975),100*(1-.467),.25) 
```

### Bayesian

We measure is $n_{pass}$, the number of students that pass. Here we measured 60

$$
n_{pass}= f*100 +   Y\\
Y \sim Binom(n=(1-f)*100,1/4)
$$
 

$$
P[f | n_{pass}] \propto P[n_{pass}\mid f]P[f] = Binom(y=n_{pass}-(1-f)*100, n=(1-f)*100,1/4)P[f]
$$

If we assume a uniform prior, the distribution looks like this (grid approx)

```{r}

grid_approx <- tibble(f = 35:75/100, p = dbinom(x= 60-(1-f)*100,size=(1-f)*100,p=1/4)) |>
  mutate(p = p/sum(p))

ggplot(grid_approx, aes(x=f,y=p)) + geom_line()
 
```

Using grid approximation (i dont think this is a case where I can use beta distribution?)

```{r}
grid_approx <- grid_approx |> mutate( cdf = cumsum(p))
grid_approx |> filter((abs(cdf - .025) < .02) | (abs(cdf - .975) < .02))
```

So the 95% range of f is from 46 to 56 

## Exercise 4.11

Working through your own example: Continuing the example from the final exercises of the
earlier chapters, perform some basic comparisons, confidence intervals, and hypothesis tests and
discuss the relevance of these to your substantive questions of interest.