# Selected Exercises
```{r}
library(tidyverse)
library(rstanarm)
library(ggplot2)
```

## Exercise 11.3 Coverage of confidence intervals:

```{r}
n <- 100
x <- runif(n,0,10)
a <- 2
b <- 3
y0 <- a + b*x  

fit_b <- \() {
  data <- tibble(x = x, y = y0+ rnorm(n))
  fit <- stan_glm(y ~ x, data = data, refresh = 0)
  list(b = coef(fit)[2], se = se(fit)[2])
}

fit_b()
```

Now fit it 1000 times.

```{r}
b_fits <- rep(0,1000)
b_se <- rep(0,1000)

for(i in seq_along(b_fits))
{
  res <- fit_b()
  b_fits[[i]] <- res$b
  b_se[[i]] <- res$se
}
```

a) True, since the errors in the regression should be approximatly normal, given the normal distribution of the errors. 

```{r}
sum(abs(b_fits - b) <= 2*b_se) 

```

b) Even though the distribution of errors is bimodal, the parameters are basically a linear combinaton of the $y_i$, so central limit theorem should apply. 

Lets check, i will draw errors from a bimodal distribution created by addign to normal distributions, one at +1 and one at -1, each with standard devitiation 0,5.

```{r}
hist(rnorm(n,0,0.5) + sample(c(rep(1,n/2),rep(-1,n/2))), breaks=20)
```

```{r}
fit_b_2 <- \() {
  noise <- rnorm(n,0,0.5) + sample(c(rep(1,n/2),rep(-1,n/2)))
  data <- tibble(x = x, y = y0 + noise)
  fit <- stan_glm(y ~ x, data = data, refresh = 0)
  list(b = coef(fit)[2], se = se(fit)[2])
}

fit_b_2()
```

Still fits fine. 

```{r}
b_fits <- rep(0,1000)
b_se <- rep(0,1000)

for(i in seq_along(b_fits))
{
  res <- fit_b_2()
  b_fits[[i]] <- res$b
  b_se[[i]] <- res$se
}

sum(abs(b_fits - b) <= 2*b_se) 
```


Lets take a look at the parameter distibution in one fit:

```{r}
noise <- rnorm(n,0,0.5) + sample(c(rep(1,n/2),rep(-1,n/2)))
data <- tibble(x = x, y = y0 + noise)
fit <- stan_glm(y ~ x, data = data, refresh = 0)
draws <- as_tibble(fit)
ggplot(data = draws, aes(x=x)) + geom_histogram()
```

Still looks 'normalish' . This (numerically) adds justification for Gelman's assertian the assumption of normality is typically barely important at all when it comes to finding regression coefficients (but what about outliers???)

## 11.6  Fitting the wrong model.

This is in a similar vein to previous, but uses t distribution which might illuminate outlier issues.

 

Sim data
```{r}
set.seed(33)
sim_dat <- \(){
  x1 <- seq(1,100)
  x2 <- rbinom(100,1,.5)
  y_t <- 3 + 0.1*x1 + 0.5*x2
  tibble(x1,x2,y = y_t + 5*rt(100, 4))
}

fit <- stan_glm(y ~ x1 + x2, data = sim_dat(), refresh = 0)
print(fit, digits = 3)
```

The true values are covered by 1 SD. 

Now to do the loop:

```{r}
n_sims = 1000
cov_int = rep(0,n_sims)
cov_x1 = rep(0,n_sims)
cov_x2 = rep(0,n_sims)

for(i in seq(1:n_sims)){
  fit <- stan_glm(y ~ x1 + x2, data = sim_dat(), refresh = 0)
  coefs <- coef(fit)
  ses <- se(fit)
  cov_int[[i]] = abs(coefs[1] - 3) <= ses[1]
  cov_x1[[i]] = abs(coefs[2] - 0.1) <= ses[2]
  cov_x2[[i]] = abs(coefs[3] - 0.5) <= ses[3]
}

c(mean(cov_int), mean(cov_x1), mean(cov_x2))

#[1] 0.679 0.666 0.675
```


This suggests that although outliers will pull a fit away , the direction they pull is random, and in the end this effects the errors of the coefficients , which stan estimates quite well even with the wrong model.


## Exercise 11.9

Note this requires Ros-Examples to be cloned up one level

```{r }
beauty <- read_csv("../Ros-Examples/Beauty/data/beauty.csv", col_types = "nnfiffff")
head(beauty)
```


```{r}
summary(beauty)
```

Most of the `course_id` are 0, I am not going to use this predictor. 

Lets start looking at eval vs beuty

```{r}
ggplot(data = beauty) + geom_point(mapping = aes(x= beauty, y= eval, color = female))
```

 
We will need several models to compare, lets start fitting everything and see what predictors might be important, including an interaction for beauty and sex.

```{r}
fit_all <- stan_glm(eval ~ beauty*female + age + minority + nonenglish + lower , data = beauty, refresh=0)
print(fit_all)
```
```{r}
loo_all <- loo(fit_all)
print(loo_all)
```
Ok I will pick 3 more models to compare. 

```{r}
fit_1 <- stan_glm(eval ~ beauty + female , data = beauty, refresh=0)
print(fit_1)
```

```{r}
loo1 <- loo(fit_1)
print(loo1)
```

```{r}
fit_2 <- stan_glm(eval ~ beauty * female , data = beauty, refresh=0)
print(fit_2)
```

```{r}
loo2 <- loo(fit_2)
print(loo2)
```

We note that the interaction did not add anything

```{r}
fit_3 <- stan_glm(eval ~ beauty + female + nonenglish, data = beauty, refresh=0)
print(fit_3)
```

```{r}
loo3= loo(fit_3)
print(loo3)
```

```{r}
loo_compare(loo_all, loo3, loo2, loo1)
```
 
The difference between the epld on this two models is not significant, so take the simpler one (fit_3) 

I am only going to look at fit_3 for pointwise predictive errors, looking at residuals

```{r}
b_with_pred = beauty |> arrange(beauty)
b_with_pred$eval_point_pred <- predict(fit_3)

ggplot(data = b_with_pred) + geom_point(aes(x=eval_point_pred, y =  eval- eval_point_pred, color = female)) + geom_abline(slope =0, intercept =0, color = "red")
```

Not sure what i am supposed to get out of that.
 


## Exercise 11.10

Here i am only going to look at fit_3 and fit_1 , which differ only in adding 'nonenglish' as a predictor.  The loo difference was barely signficant, so it seems useful to compare these using  manual CV:

(a) Randomly partition the data into five parts using the sample function in R.
(b) For each part, re-fitting the model excluding that part, then use each fitted model to predict
the outcomes for the left-out part, and compute the sum of squared errors for the prediction.
(c) For each model, add up the sum of squared errors for the five steps in (b). Compare the
different models based on this fit.

```{r}
folds = 5
n <- nrow(beauty)
shuffled_index <- sample(1:n, n, replace = FALSE)

residuals_1 <- rep(0,5)

residuals_3 <- rep(0,5)

start = 1
for(i in 1:folds)
{
  end = start + n/5 - 1
  data <- beauty[-(round(start):round(end)),]
  print(paste0("Fold ",i, ", length", nrow(data)))
  fit_1 <- stan_glm(eval ~ beauty + female , data = data, refresh=0)
  residuals_1[[i]] <- sum((predict(fit_1) - data$eval)^2)
  
  fit_3 <- stan_glm(eval ~ beauty + female + nonenglish , data = data, refresh=0)
  residuals_3[[i]] <- sum((predict(fit_3) - data$eval)^2)
  
  start = end + 1
}
```

That code is very non-tidy.  


```{r}
print(mean(residuals_1))
print(mean(residuals_3))

```

Fit three is still better. 

```{r}
sqrt(sd(residuals_3)^2/5 + sd(residuals_1)^2/5)
```