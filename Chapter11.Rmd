# Selected Exercises
```{r}
library(tidyverse)
library(rstanarm)
library(ggplot2)
```

## Exercise 11.3 Coverage of confidence intervals:

```{r}
n <- 100
x <- runif(n,0,10)
a <- 2
b <- 3
y0 <- a + b*x  

fit_b <- \() {
  data <- tibble(x = x, y = y0+ rnorm(n))
  fit <- stan_glm(y ~ x, data = data, refresh = 0)
  list(b = coef(fit)[2], se = se(fit)[2])
}

fit_b()
```

Now fit it 1000 times.

```{r}
b_fits <- rep(0,1000)
b_se <- rep(0,1000)

for(i in seq_along(b_fits))
{
  res <- fit_b()
  b_fits[[i]] <- res$b
  b_se[[i]] <- res$se
}
```

a) True, since the errors in the regression should be approximatly normal, given the normal distribution of the errors. 

```{r}
sum(abs(b_fits - b) <= 2*b_se) 

```

b) Even though the distribution of errors is bimodal, the parameters are basically a linear combinaton of the $y_i$, so central limit theorem should apply. 

Lets check, i will draw errors from a bimodal distribution created by addign to normal distributions, one at +1 and one at -1, each with standard devitiation 0,5.

```{r}
hist(rnorm(n,0,0.5) + sample(c(rep(1,n/2),rep(-1,n/2))), breaks=20)
```

```{r}
fit_b_2 <- \() {
  noise <- rnorm(n,0,0.5) + sample(c(rep(1,n/2),rep(-1,n/2)))
  data <- tibble(x = x, y = y0 + noise)
  fit <- stan_glm(y ~ x, data = data, refresh = 0)
  list(b = coef(fit)[2], se = se(fit)[2])
}

fit_b_2()
```

Still fits fine. 

```{r}
b_fits <- rep(0,1000)
b_se <- rep(0,1000)

for(i in seq_along(b_fits))
{
  res <- fit_b_2()
  b_fits[[i]] <- res$b
  b_se[[i]] <- res$se
}

sum(abs(b_fits - b) <= 2*b_se) 
```


Lets take a look at the parameter distibution in one fit:

```{r}
noise <- rnorm(n,0,0.5) + sample(c(rep(1,n/2),rep(-1,n/2)))
data <- tibble(x = x, y = y0 + noise)
fit <- stan_glm(y ~ x, data = data, refresh = 0)
draws <- as_tibble(fit)
ggplot(data = draws, aes(x=x)) + geom_histogram()
```

Still looks 'normalish' . This (numerically) adds justification for Gelman's assertian the assumption of normality is typically barely important at all when it comes to finding regression coefficients (but what about outliers???)

## 11.6  Fitting the wrong model.

This is in a similar vein to previous, but uses t distribution which might illuminate outlier issues.

 

Sim data
```{r}
set.seed(33)
sim_dat <- \(){
  x1 <- seq(1,100)
  x2 <- rbinom(100,1,.5)
  y_t <- 3 + 0.1*x1 + 0.5*x2
  tibble(x1,x2,y = y_t + 5*rt(100, 4))
}

fit <- stan_glm(y ~ x1 + x2, data = sim_dat(), refresh = 0)
print(fit, digits = 3)
```

The true values are covered by 1 SD. 

Now to do the loop:

```{r}
n_sims = 1000
cov_int = rep(0,n_sims)
cov_x1 = rep(0,n_sims)
cov_x2 = rep(0,n_sims)

for(i in seq(1:n_sims)){
  fit <- stan_glm(y ~ x1 + x2, data = sim_dat(), refresh = 0)
  coefs <- coef(fit)
  ses <- se(fit)
  cov_int[[i]] = abs(coefs[1] - 3) <= ses[1]
  cov_x1[[i]] = abs(coefs[2] - 0.1) <= ses[2]
  cov_x2[[i]] = abs(coefs[3] - 0.5) <= ses[3]
}

c(mean(cov_int), mean(cov_x1), mean(cov_x2))

#[1] 0.679 0.666 0.675
```


This suggests that although outliers will pull a fit away , the direction they pull is random, and in the end this effects the errors of the coefficients , which stan estimates quite well even with the wrong model.


## Exercise 11.9


## Exercise 11.10
