# Selected Exercises
```{r}
library(tidyverse)
library(rstanarm)
library(ggplot2)
```

## 10.1 Regression with interactions

Simulate 100 data points for $y = b_0 + b_1 X + b_2 z + b_3 x z$. 
This block sets up specifications as in the problem statement;
```{r}
set.seed(33)
N = 100
b = c(1,2,-1,-2)
errors = rnorm(N,0,3)
df = tibble(z_n = rbinom(N,1,0.5),
            z = factor(z_n),  # to make plots nice
            x = rnorm(N,z_n,1),
            y = b[1]+b[2]*x + b[3]*z_n + b[4]*x*z_n + errors)
```
```{r}
p1 = ggplot(data= df, aes(x=x,y=y, color = z)) + geom_point()
p1

```

```{r}
mod1 <- stan_glm(y ~ x + z, data  = df, refresh =0)
print(mod1)
```

```{r}
xt <- seq(-3,3,.1)
fit_df <- tibble(x=xt, y0 = coef(mod1)[1]+ coef(mod1)[2]*xt, y1=y0 + coef(mod1)[3]) |>
          pivot_longer(-x, values_to = "y") |> mutate(z= factor(case_when(name=="y0" ~ 0,TRUE ~1)))
```


```{r}
p1 + 
  geom_line(data = fit_df , mapping = aes(x = x,y = y, color=z)) 
```

```{r}
mod2 <- stan_glm(y ~ x + z + x:z, data  = df, refresh =0)
print(mod2)
```

```{r}
xt <- seq(-3,3,.1)
fit_df_2 <- tibble(x=xt, y0 = coef(mod2)[1]+ coef(mod2)[2]*xt,
                         y1=y0 + coef(mod2)[3] + coef(mod2)[4]*xt) |>
          pivot_longer(-x, values_to = "y") |> mutate(z= factor(case_when(name=="y0" ~ 0,TRUE ~1)))
```


```{r}
p1 + 
  geom_line(data = fit_df_2 , mapping = aes(x = x,y = y, color=z)) 
```


Compare to input:
```{r}
b
```
```{r}
coef(mod2)
```

Consistent.

## 10.3

In this exercise and the next, you will simulate two variables that are statistically independent of each other to see what happens when we run a regression to predict one from the other.

```{r}
df <- tibble(var1 = rnorm(1000,0,1),
             var2 = rnorm(1000,0,1))
mod <- stan_glm( var2 ~ var1, data = df, refresh =0)
print(mod, digits =3)

```

I suppose   one would conclude there is no significant effect!

## 10.4

```{r}
z_scores <- rep(NA, 100)
for (k in 1:100) {
   var1 <- rnorm(1000, 0, 1)
   var2 <- rnorm(1000, 0, 1)
   fake <- data.frame(var1, var2)
   fit <- stan_glm(var2 ~ var1, data=fake, refresh = 0)
   if(k%%10 == 0) print(k)
   z_scores[k] <- coef(fit)[2] / se(fit)[2]
}

mean(abs(z_scores)>2)
```

So 5% of the time,  we see a spurious correlation effect.  I suspect this can be analyticaly derived, but i am too lazy to work it out. Would need to work out the  distribution of $\rho$ as a random variable.

## 10.9

```{r}
hibbs <- read.table('hibbs.dat', header= TRUE)
mod1 <- stan_glm('vote ~ growth', data = hibbs, refresh=0)
print(mod1)
```


a) Create a collinear variable to growth

```{r}
hibbs <- hibbs |> mutate(cl_growth = growth)
hibbs |> ggplot(aes(x=cl_growth, y= growth)) + geom_point()
```

b)

```{r}
mod2 <- stan_glm('vote ~ growth + cl_growth', data = hibbs, refresh = 0)
print(mod2)
```

We can see that each predictor has very large MAD_SD, but this is just because of the correlation between these two parameters:

```{r}
ggplot(data = as.tibble(mod2), aes(x= growth, y = cl_growth))+ geom_point()
```

c)
Now we want a nearly colinear case. If we have two variables x and y, and $y = x + \epsilon$ where $\epsilon$ is normal with standard deviation $\sigma$, the correlation coefficent is:

$$
Cov(X,Y) = Cov(X, X + \epsilon) = Var(X) + Cov(X,\epsilon) = Var(X)\\
\rho = \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}} = \frac{\sigma_X}{\sqrt{\sigma_X^2+\sigma_{\epsilon}^2}} 
$$

Solving for $\sigma_{\epsilon}$

$$
\sigma_{\epsilon} = \sigma_Y\sqrt{1/\rho^2 - 1}
$$
```{r}
sig_e = sd(hibbs$growth)*sqrt(1/.9^2 - 1)
sig_e
```

Using this gave too small of a sample correlation coefficient, due to spurious correlation between the random noise and the data.

Using sd = 0.7 gave approximately the right correlation coefficient. 

```{r}
hibbs <- hibbs |> mutate(nl_growth = growth + rnorm(length(growth),mean = 0,sd = 0.7))
cor(hibbs$growth,hibbs$nl_growth)
```

```{r}
hibbs |> ggplot(aes(x=nl_growth, y= growth)) + geom_point()
```

```{r}
mod3 <- stan_glm('vote ~ growth + nl_growth', data = hibbs, refresh = 0)
print(mod3)
```

The errors are still larger then with one of the colinear predictors, but smaller then previously. Again these coefficients are correlated which we can see:


```{r}
ggplot(data = as.tibble(mod3), aes(x= growth, y = nl_growth))+ geom_point()
```

If you see this kind of thing, this might tell you to consider linear combinations of the predictors, in this case growth + nl_growth and growth- nl_growth.  (The difference in this case is just noise, but in general it could be something.).  

