# Chapter 17 


```{r}
library(rstanarm)
invlogit <- plogis
library(tidyverse)
library(ggplot2)
```
## Exercises 

### 17.2

Copied from section Poststrat2_tv.RMD.  

I decided that looking at all the census data was too tedius, but I decided to at least work through the tidyverse version of 17.2

#### Creating the artificial world

Simulation parameters:

* `pop_prop`: Proportion of the population
* `response`: Response rate relative to `response_baseline`
* `coef`: Coefficient of formula for probability of "Yes" response to survey

```{r}
pop <- 250e6
response_baseline <- 0.1
coef_intercept <- 0.6

params <- 
  tribble(
    ~var,        ~value,     ~pop_prop, ~response, ~coef,
    "sex",       "Female",        0.52,       1.0,     0,
    "sex",       "Male",          0.48,       0.8,  -0.2,
    "age",       "18 - 29",       0.20,       1.0,     0,
    "age",       "30 - 44",       0.25,       1.2,  -0.2,
    "age",       "45 - 64",       0.30,       1.6,  -0.3,
    "age",       "65+",           0.25,       2.5,  -0.4,
    "ethnicity", "White",         0.70,       1.0,     0,
    "ethnicity", "Black",         0.10,       0.8,   0.6,
    "ethnicity", "Hispanic",      0.10,       0.7,   0.3,
    "ethnicity", "Other",         0.10,       0.6,   0.3
  )
```

Function to return simulation parameter.

```{r}
param <- function(var_, value_, param) {
  params %>% 
    filter(var == var_, value == value_) %>% 
    pull({{ param }})
}

param("sex", "Female", pop_prop)
```

Poststratification cells with assumed population, response rate, and probability of "Yes" response to survey.

```{r}
poststrat <- 
  expand_grid(
    sex = c("Female", "Male"),
    age = c("18 - 29", "30 - 44", "45 - 64", "65+"),
    ethnicity = c("White", "Black", "Hispanic", "Other")
  ) %>% 
  mutate(across(c(sex, age, ethnicity), fct_inorder)) %>% 
  rowwise() %>% 
  mutate(
    n = 
      pop *
      param("sex", sex, pop_prop) *
      param("age", age, pop_prop) *
      param("ethnicity", ethnicity, pop_prop),
    response =
      response_baseline * 
      param("sex", sex, response) *
      param("age", age, response) *
      param("ethnicity", ethnicity, response),
    yes_prob =
      plogis(
        coef_intercept +
          param("sex", sex, coef) +
          param("age", age, coef) +
          param("ethnicity", ethnicity, coef)
      )
  ) %>% 
  ungroup()
```


#### Sample

We then sample from the assumed population with the assumed response rate.

```{r}
set.seed(457)

n_people <- 1000

people <- 
  sample(
    nrow(poststrat),
    size = n_people,
    replace = TRUE, 
    prob = poststrat$n * poststrat$response
  )
```

Check that each cell was sampled.

```{r}
setequal(seq_len(nrow(poststrat)), people)
```

Add proportion of population and proportion of sample for each cell in post stratification table.

```{r}
poststrat <- 
  poststrat %>% 
  mutate(
    n_prop = n / sum(n),
    cell = row_number()
  ) %>% 
  left_join(
    tibble(cell = people) %>% count(cell, name = "n_sample"),
    by = "cell"
  ) %>% 
  mutate(sample_prop = n_sample / n_people) %>% 
  select(!c(cell, n_sample))
```

Simulate survey data.

```{r}
set.seed(435)

data <- 
  poststrat %>% 
  slice(people) %>% 
  mutate(y = rbinom(n(), size = 1, prob = yes_prob)) %>% 
  select(y, sex, age, ethnicity)
```

#### Performing regression and poststratification

First, we fit a logistic regression, predicting the survey response given sex, age, and ethnicity, with no interaction:

```{r}
set.seed(907)

fit <- 
  stan_glm(
    y ~ sex + age + ethnicity,
    family = binomial(link = "logit"),
    data = data,
    refresh = 0
  )

fit
```

Estimate the proportion of "Yes" responses for each cell in the poststratification table.

```{r}
poststrat <- 
  poststrat %>% 
  mutate(yes_pred = predict(fit, type = "response", newdata = .))

```

Finally, we poststratify to estimate the proportion of the entire population that would answer "Yes" to the survey:

```{r}
poststrat %>% 
  summarize(yes_pop = sum(yes_pred * n_prop))
```

The above gives us the point estimate; to get inferential uncertainty, we can work with the matrix of posterior simulations:

```{r}
tibble(
  yes_pop = posterior_epred(fit, newdata = poststrat) %*% poststrat$n_prop
) %>% 
  summarize(across(yes_pop, list(mean = mean, sd = sd)))
```

 


### 17.5

```{r}
data <- as_tibble(mpg) |> select(cty, displ) |> mutate(mpg = cty) |> select(-cty)
data |> ggplot(aes(x= log(displ), y=mpg)) + geom_point()
```

```{r}
data <- data |> mutate(ldispl = log(displ)) |> select(-displ)
```


#### Full regression

```{r}
set.seed(42)
fit = stan_glm(mpg ~  ldispl, data= data, refresh = 0)
fit
```

#### a) Gen Missing data 

```{r}
mid_mpg <- 20

p_miss <- \(mpg) {
  invlogit(-1-.25*(mpg-mid_mpg))
}

tibble(mpg = seq(10:35), p= p_miss(mpg)) |>
  ggplot(aes(x= mpg,y =p)) + geom_point()

```

```{r}
set.seed(337)
data_miss <- data |>
  rowwise() |>
   mutate(missing = rbinom(1, size = 1, p = p_miss(mpg)) )|>
   ungroup() |>
   mutate(ldispl = if_else(as.logical(missing), NA, ldispl)) |>
   select(-missing)
```

How many are missing? 


```{r}
sum(is.na(data_miss$ldispl))/nrow(data_miss)
```



#### b) Regression of x on y:

```{r}
set.seed(42)
impute_full = stan_glm(ldispl ~ mpg, data= data, refresh = 0)
print(impute_full, digits =4)
```
```{r}
set.seed(42)
impute = stan_glm(ldispl ~ mpg, data= data_miss, refresh = 0)
print(impute, digits = 4)
```

As claimed the two fits are somewhat consistent.  

#### c) compare complete case  to original fit:

 
```{r}
fit_miss = stan_glm(mpg ~ ldispl, data= data_miss, refresh = 0)
fit_miss
```

Not really inconsistent?

####  d) use impute fit to randomly impute missing data. 

```{r}
pred_miss <- posterior_predict(impute, newdata = data_miss |> select(mpg), draws = 1)[1,]
data_impute <- data_miss |> mutate(imputed = is.na(ldispl), ldispl = if_else(imputed, pred_miss, ldispl))
data_impute |> ggplot(aes(x= ldispl, y=mpg, color = imputed)) + geom_point()
```

Looks realistic i guess

```{r}
fit_impute = stan_glm(mpg ~  ldispl, data= data_impute, refresh = 0)
fit_impute
```

Lets do this n times and average

```{r}
nsims = 100
slope = rep(NA, nsims)
mad_sd = rep(NA, nsims)
for( i in seq(nsims)){
  pred_miss <- posterior_predict(impute, newdata = data_miss |> select(mpg), draws = 1)[1,]
  data_impute <- data_miss |> mutate(imputed = is.na(ldispl), ldispl = if_else(imputed, pred_miss, ldispl))
  fit_impute = stan_glm(mpg ~  ldispl, data= data_impute, refresh = 0)
  slope[i] = coef(fit_impute)[2]
  mad_sd[i] = se(fit_impute)[2]
}

mean(slope)


```

USing formula from page 326, neglecting 1/M term:

```{r}
sqrt(mean(mad_sd^2)+ var(slope))
```

Final estimate is -9.3  +- 0.6,  which is consistant with the original (-9.4 +- 0.4)
Interesting? Discuss
